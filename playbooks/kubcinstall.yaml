# ## Install K8S On Ubuntu
# The initial chores before this (playbook) involve:
# 
# - Create service account (e.g. with "user" module)
# - Adding service account to sudo group
# - Copying local ansible user public key to service acct
#   (e.g. using ansible `authorized_key` module)
# 
# ## Host inventory vars
# 
# - On master set `k8smaster=1`
# - No need to set anything on Nodes/Workers
# 
# ## Playbook Vars
# 
# Choose carefully the version of k8s (k8s_ver) for e.g. network plugin compatibility.
# 
# ## Running
# Typical extra vars: '{"k8s_user":"kubadm", "k8s_user_home":"/home_local/kubadm", "k8s_ver": "1.21.9-00"}'
# Fixed: The error was: error while evaluating conditional (k8smaster): 'k8smaster' is undefined (fixed by: k8smaster is defined)
# 
# 
# ## Verifying
# - Log on to master: ssh ubuntu@master_ip
# - Execute: kubectl get nodes
# 
# ## Refs
# -  https://phoenixnap.com/kb/install-kubernetes-on-ubuntu
# - https://zhuanlan.zhihu.com/p/40931670 (Chinese, use Translate)
#  - /etc/default/kubelet: KUBELET_KUBEADM_EXTRA_ARGS=--cgroup-driver=<value>
#  - Lots of other relevant points
# - Ubu 20.04, Nvidia and Calico: https://zhuanlan.zhihu.com/p/138554103
# 
- name: Install Kubernetes Cluster
  hosts: '{{ host }}'
  become: yes
  vars:
    # Built in ... ansible_distribution_release or ansible_lsb.codename
    # Distros: xenial=16.04, bionic=18.04, focal=20.04 jammy=22.04
    k8s_distro: 'bionic'
    # In case new docker will be needed (?)
    dkr_apt_key_url: https://download.docker.com/linux/ubuntu/gpg
    # distro name: $(lsb_release -cs)
    dkr_apt_repo_line: "deb https://download.docker.com/linux/ubuntu {{ ansible_lsb.codename }} stable
    # 
    k8s_apt_key_url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
    # Orig: kubernetes-xenial main
    k8s_apt_repo_line: "deb http://apt.kubernetes.io/ kubernetes-{{ ansible_lsb.codename }} main"
    # k8s_ver: 1.14.0-00
    # latest for 18.04 (as of 2022-02):
    # k8s_ver: 1.23.3-00
    # Should use (for 18.04, as of 22-02)
    k8s_ver: 1.21.9-00
    # Flannel Default 10.244.0.0/16. 
    k8s_cluster_cidr: '10.244.0.0/16'
    k8s_user: ubuntu
    # TODO: use in tasks !
    k8s_user_home: /home/ubuntu
    # Github repo: https://github.com/flannel-io/flannel/blob/master/Documentation/kube-flannel.yml
    # ()
    # k8s_flannel_yaml: https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml
    # 221a83cab893a4a724aaff0bb53fbfd14a7724e4 Jul 24, 2020
    k8s_flannel_yaml: https://raw.githubusercontent.com/coreos/flannel/221a83cab893a4a724aaff0bb53fbfd14a7724e4/Documentation/kube-flannel.yml
  tasks:
    # Add K8S install pkg repos
    # curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
    - name: Add Google K8S apt-key
      apt_key:
        url: '{{ k8s_apt_key_url }}'
        state: present
    # sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-... main"
    - name: Add Kubernetes APT Repository
      apt_repository:
        repo: '{{ k8s_apt_repo_line }}'
        state: present
        filename: 'kubernetes'
    # Must do on docker configs (will this sustain over docker.io install ?)
    # The cgroupdriver must match between docker and k8s (See: misconfiguration: kubelet cgroup driver: ...)
    # Set as *exclusive content of /etc/docker/daemon.json
    # NOTE: /var/lib/kubelet/config.yaml is supposed to have "cgroupDriver" (at least in some versions, e.g. around 1.11.0)
    # See: https://github.com/kubernetes/kubernetes/issues/65863
    - name: Set cgroupdriver to match (docker vs. k8s)
      copy:
        content: '{ "exec-opts": ["native.cgroupdriver=cgroupfs"] }\n'
        dest: /etc/docker/daemon.json
        backup: yes
	force: yes
        mode: '644'
    # Possible errors:
    # - misconfiguration: kubelet cgroup driver: ...x
    # - failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet config file \"/var/lib/
    - name: Master/WorkerNode Common installs
      apt:
        update_cache: yes
        state: present
        force_apt_get: yes
        install_recommends: no
        name: ["docker.io", "apt-transport-https", "kubelet={{ k8s_ver }}", "kubeadm={{ k8s_ver }}"]
    - name: Install kubectl on Master
      apt:
        name: "kubectl={{ k8s_ver }}"
        state: present
        force: yes
        install_recommends: no
      when: k8smaster is defined
    #  "stderr": "swapoff: –a: swapoff failed: No such file or directory",
    #- name: Disable swap for K8S Initialization
    #  shell: /sbin/swapoff –a
    # Suggested by error msgs: -v=5  --ignore-preflight-errors=all
    # "The connection to the server 10.75....:6443 was refused - did you specify the right host or port?"
    # Host tried (from master) was master
    # kube-apiserver uses port 6443
    # failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet con...
    # misconfiguration: kubelet cgroup driver: \"systemd\" is different from docker cgroup driver: \"cgroupfs\"
    # https://stackoverflow.com/questions/45708175/kubelet-failed-with-kubelet-cgroup-driver-cgroupfs-is-different-from-docker-c
    #  journalctl -u kubelet
    # Suggested in  /etc/docker/daemon.json   "exec-opts": ["native.cgroupdriver=cgroupfs"],
    - name: Initialize the Cluster (on master)
      # --image-repository
      shell: kubeadm init --pod-network-cidr={{ k8s_cluster_cidr }} >> cluster_initialized.txt
      args:
        chdir: $HOME
        creates: cluster_initialized.txt
      when: k8smaster is defined
    # kubeadm init On nodes (to e.g. create /var/lib/kubelet/config.yaml) ?
    # Can we not impersonate, but say user: ... (like in copy) here. YES, BUT...
    # become/become_user is better at detecting users (possibly custom) homedir
    # Also: {{ ansible_env.HOME }} on https://docs.ansible.com/ansible/latest/reference_appendices/faq.html
    - name: Create .kube directory
      become: yes
      become_user: '{{ k8s_user }}'
      file:
        path: $HOME/.kube
        #path: '/home/{{ k8s_user }}/.kube'
        #path: '{{ k8s_user_home }}/.kube'
        state: directory
        mode: 0755
        #owner: '{{ k8s_user }}'
      when: k8smaster is defined
    # Is the file heavily protected, can we symlink ?
    - name: Copy admin.conf to user's kube config
      copy:
        src: /etc/kubernetes/admin.conf
        # NOTE: This will not work for custom homedirs

        # dest: '/home/{{ k8s_user }}/.kube/config'
        dest: '{{ k8s_user_home }}/.kube/config'
        remote_src: yes
	force: yes
        owner: '{{ k8s_user }}'
      when: k8smaster is defined
    # Gets Multi-doc with kind(s): ClusterRole,ClusterRoleBinding,ServiceAccount,
    # DaemonSet(amd64),DaemonSet(arm64),DaemonSet(arm),DaemonSet(ppc),DaemonSet(s390x)
    # DaemonSet:s (should be) installed by (e.g.) nodeSelector: beta.kubernetes.io/arch: amd64
    # Complaints (Kub. 1.23.3-00) about kinds: ClusterRole, ClusterRoleBinding (and
    # DaemonSet in later YAML manifest). Have these kinds been deprecated with very recent 1.23 k8s ?
    - name: install Pod (flannel) network (by k8s YAML apply)
      become: yes
      become_user: '{{ k8s_user }}'
      shell: 'kubectl apply -f {{ k8s_flannel_yaml }} >> pod_network_setup.txt'
      args:
        chdir: $HOME
        creates: pod_network_setup.txt
      when: k8smaster is defined
    ###### Worker Nodes ########################
    - name: Generate join-token on master
      shell: kubeadm token create --print-join-command
        register: join_command
      when: k8smaster is defined
    - name: Run kubeadm init on Workers
      shell: kubeadm init
      when: k8smaster is not defined
    # Seems must set_fact as intermediate step.
    # Seems: This would need to be in a single host play (e.g. host: master)
    # Needs to be in a multi-play playbook YAML
    # Needs to be recalled (in second, worker play) by: shell: "{{ hostvars['master'].join_command }}
    #- name: Set Join command fact
    #  set_fact:
    #    join_command: "{{ join_command_raw.stdout_lines[0] }}"
    #  Must run join on workers
    # kubeadm join --discovery-token abcdef.1234567890abcdef --discovery-token-ca-cert-hash sha256:1234..cdef 1.2.3.4:6443
      when: k8smaster
    # This MAY need to be a play in itself
    # gather_facts: false
    - name: get join command
      shell: 'kubeadm token create --print-join-command'
      register: join_command_raw
      when: k8smaster
    # set_fact Sets *on the host* being executed, then avail. in (e.g.) hostvars[inventory_hostname][varname]
    - name: Set join command
      set_fact:
        join_command: "{{ join_command_raw.stdout_lines[0] }}"
      when: k8smaster
    # Continue by executing join_command on Workers.
    # Note 'master' here is the inventory_hostname for master (first token on hosts line)
    #- name: join cluster
    #  shell: "{{ hostvars['master'].join_command }} >> node_joined.txt"
    #  args:
    #    chdir: $HOME
    #    creates: node_joined.txt
    - name: Run kubeadm join ... on workers
      # Example (In newer (already at least in 1.18): --token, not --discovery-token)
      #shell: kubeadm join --discovery-token abcdef.1234567890abcdef --discovery-token-ca-cert-hash sha256:1234..cdef 1.2.3.4:6443
      shell: '{{ join_command.stdout_lines[0] }}'
    # Need to restart worker kubelet ?
    - name: Worker Kubelet restart
      # shell: echo Hello
      shell: systemctl daemon-reload ; systemctl restart kubelet
      when: k8smaster is not defined
